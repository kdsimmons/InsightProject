{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import bs4\n",
      "import re, numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import requests, urllib2\n",
      "import tweepy\n",
      "import pymysql as mdb\n",
      "\n",
      "%matplotlib inline\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "chosen_disease = \"Colon cancer\"\n",
      "clean_disease_name = ' '.join(chosen_disease.lower().split())\n",
      "# need to allow synonymous names (e.g. colon vs colorectal cancer)\n",
      "\n",
      "def scrape_bbb_urls(disease):\n",
      "    # import URL into beautiful soup object\n",
      "    \n",
      "    dis_as_str = '+'.join(disease.split())\n",
      "    give_org_urls = [\"http://give.org/search/?term=\" + dis_as_str + \"&location=&FilterAccredited=false&tobid=\"]\n",
      "    #, \n",
      "     #                \"http://give.org/search/?page=2&term=multiple+sclerosis&location=&FilterAccredited=false&tobid=\", \n",
      "      #               \"http://give.org/search/?page=3&term=multiple+sclerosis&location=&FilterAccredited=false&tobid=\"]\n",
      "    bbb_soups = []\n",
      "    for give_org_url in give_org_urls:\n",
      "        response = requests.get(give_org_url)\n",
      "        bbb_soups.append(bs4.BeautifulSoup(response.text))\n",
      "        \n",
      "    return bbb_soups\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 61
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def clean_bbb_info(bbb_soups, disease):\n",
      "    # pull out charity links and info\n",
      "    # NOTE: May need permission to include links to BBB. I didn't find any info on restrictions in pulling information.\n",
      "    charities = []\n",
      "    for soup in bbb_soups:\n",
      "        search_tables = soup.select('div.search table')\n",
      "        if len(search_tables) == 0:\n",
      "            continue\n",
      "        bbb_table = soup.select('div.search table')[0].tbody \n",
      "        for row in bbb_table.findAll('tr'):\n",
      "            charity_data = {}\n",
      "            charity_data['disease'] = disease\n",
      "            charity_data['name'] = row.select('a.charity-link')[0].get_text()\n",
      "            charity_data['bbb_link'] = row.select('a.charity-link')[0].attrs.get('href')\n",
      "            accred_seal = row.select('img.charity-search-seal')\n",
      "            if accred_seal:\n",
      "                charity_data['bbb_accred'] = accred_seal[0].attrs.get('alt')\n",
      "            else:\n",
      "                charity_data['bbb_accred'] = row.select('div.accreditation-mobile')[0].get_text()\n",
      "            charity_data['address'] = row.select('div.accreditation-mobile')[0].next_sibling.strip()\n",
      "            charity_data['city'] = row.select('div.accreditation-mobile')[0].next_sibling.next_sibling.next_sibling.strip()\n",
      "            \n",
      "            charities.append(charity_data)\n",
      "            \n",
      "    return charities\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bbb_soups = scrape_bbb_urls(clean_disease_name)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 62
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(bbb_soups)\n",
      "print clean_disease_name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "colon cancer\n"
       ]
      }
     ],
     "prompt_number": 64
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bbb_charities = clean_bbb_info(bbb_soups)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(bbb_charities)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 22,
       "text": [
        "3"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print bbb_charities[2]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'city': u'Alexandria, VA 22314', 'name': u'Colorectal Cancer Coalition', 'disease': 'colorectal cancer', 'address': u'1414 Prince Street, Suite 204', 'bbb_link': 'http://www.give.org/charity-reviews/national/cancer/colorectal-cancer-coalition-in-alexandria-va-5314', 'bbb_accred': u'\\nAccredited: Yes\\n'}\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_charity_links(charities):\n",
      "    # Use BBB pages to get links to each charity's website\n",
      "    \n",
      "    for charity in charities:\n",
      "        soup = bs4.BeautifulSoup(requests.get(charity['bbb_link']).text)\n",
      "        if len(soup.select('div.charity-contact')) > 0:\n",
      "            charity['link'] = soup.select('div.charity-contact')[0].a.attrs.get('href')\n",
      "        elif len(soup.select('div.charity-detail-text')) > 0:\n",
      "            charity['link'] = soup.select('div.charity-detail-text')[0].a.attrs.get('href')\n",
      "        elif len(soup.select('a#ctl00_ContentPlaceHolder1_aWebUrl')) > 0:\n",
      "            charity['link'] = soup.select('a#ctl00_ContentPlaceHolder1_aWebUrl')[0].attrs.get('href')\n",
      "        else:\n",
      "            charity['link'] = ''\n",
      "    # TO DO: should add text search for websites printed as text rather than link\n",
      "    \n",
      "    return charities"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "charities_with_links = get_charity_links(bbb_charities)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(charities_with_links)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 25,
       "text": [
        "3"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def scrape_social_media(charities):\n",
      "    # Find Facebook likes and Twitter followers for each charity\n",
      "    # TO DO: Find the right FB/Twitter account, e.g. for National MS Society chapters.\n",
      "    for charity in charities:\n",
      "        if charity['link'] == '':\n",
      "            charity['facebook_link'] = ''\n",
      "            charity['twitter_link'] = ''\n",
      "        else: \n",
      "            try:\n",
      "                page = urllib2.urlopen(urllib2.Request(charity['link'], headers={'User-Agent' : 'Mozilla/5.0 (Windows; U; Windows NT 5.1; it; rv:1.8.1.11) Gecko/20071127 Firefox/2.0.0.11'}))\n",
      "                target_soup = bs4.BeautifulSoup(page.read(), 'html.parser')\n",
      "            except:\n",
      "                print charity['name']\n",
      "                continue\n",
      "    \n",
      "            try:\n",
      "                fblink = target_soup.select('a[href*=facebook.com]')[0].attrs.get('href')\n",
      "            except IndexError:\n",
      "                fblink = ''\n",
      "            charity['facebook_link'] = fblink \n",
      "            \n",
      "            try:\n",
      "                twlink = target_soup.select('a[href*=twitter.com]')[0].attrs.get('href')\n",
      "            except IndexError:\n",
      "                twlink = ''\n",
      "            charity['twitter_link'] = twlink\n",
      "            \n",
      "    return charities\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "charities_with_social = scrape_social_media(charities_with_links)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(charities_with_social)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 28,
       "text": [
        "3"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_twitter_followers(charities):\n",
      "    # import authorization info \n",
      "    twauth = pd.DataFrame.from_csv('/home/kristy/Documents/auth_codes/twitter_access.csv')\n",
      "    auth = tweepy.OAuthHandler(twauth.consumer_key['twitter'], twauth.consumer_secret['twitter'])\n",
      "    auth.set_access_token(twauth.access_token['twitter'], twauth.access_secret['twitter'])\n",
      "    \n",
      "    api = tweepy.API(auth)\n",
      "    \n",
      "    for charity in charities:\n",
      "        if charity['twitter_link'] == '':\n",
      "            charity['twitter_followers'] = 0\n",
      "        else:\n",
      "            try:\n",
      "                twitterid = re.findall('https://twitter.com/(.*)', charity['twitter_link'])[0]\n",
      "                user = api.get_user(twitterid)\n",
      "                charity['twitter_followers'] = user.followers_count\n",
      "            except:\n",
      "                charity['twitter_followers'] = 0\n",
      "                continue\n",
      "                \n",
      "    return charities\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "char_with_twitter = get_twitter_followers(charities_with_social)\n",
      "len(char_with_twitter)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 32,
       "text": [
        "3"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_char_nav_info(dictlist): \n",
      "    for charity in dictlist:\n",
      "        # get link to Charity Navigator's website on this charity\n",
      "        words = '+'.join(charity['name'].split())\n",
      "        url = ('https://www.charitynavigator.org/index.cfm?keyword_list=' + words \n",
      "               + '&nameonly=1&Submit2=Search&bay=search.results')\n",
      "        response = requests.get(url)\n",
      "        soup = bs4.BeautifulSoup(response.text)\n",
      "        if len(soup.select('p.rating')) > 0:\n",
      "            charity['cn_link'] = soup.select('p.rating')[0].a.attrs.get('href')\n",
      "            charity['cn_rated'] = 'Rated'\n",
      "        else:\n",
      "            url = ('https://www.charitynavigator.org/index.cfm?keyword_list=' + words \n",
      "                   + '&nameonly=1&Submit2=Search&bay=search.results2')\n",
      "            response = requests.get(url)\n",
      "            soup = bs4.BeautifulSoup(response.text)\n",
      "            if len(soup.select('p.orgname')) > 0:\n",
      "                charity['cn_link'] = soup.select('p.orgname')[0].a.attrs.get('href')\n",
      "                charity['cn_rated'] = 'Unrated'                \n",
      "            else:\n",
      "                print charity['name'] + \": No Charity Navigator link.\"\n",
      "                charity['cn_link'] = ''\n",
      "                charity['cn_rated'] = ''\n",
      "            charity['cn_overall'] = float('nan')\n",
      "            charity['cn_financial'] = float('nan')\n",
      "            charity['cn_acct_transp'] = float('nan')\n",
      "            continue\n",
      "        \n",
      "        # get Charity Navigator data \n",
      "        charsoup = bs4.BeautifulSoup(requests.get(charity['cn_link']).text)\n",
      "        strongtags = charsoup.select('strong')\n",
      "        overall_cells = []\n",
      "        fin_cells = []\n",
      "        acct_cells = []\n",
      "        for tg in strongtags:\n",
      "            if tg.text == \"Overall\":\n",
      "                overall_cells.append(float(tg.parent.nextSibling.nextSibling.text))\n",
      "            elif tg.text == \"Financial\":\n",
      "                fin_cells.append(float(tg.parent.nextSibling.nextSibling.text))\n",
      "            elif tg.text == \"Accountability & Transparency\":\n",
      "                acct_cells.append(float(tg.parent.nextSibling.nextSibling.text))\n",
      "        # include overall rating\n",
      "        if len(overall_cells) == 1:\n",
      "            charity['cn_overall'] = overall_cells[0]\n",
      "        elif len(overall_cells) > 1:\n",
      "            print charity['name'] + \": Too many candidate cells for Overall.\"\n",
      "            charity['cn_overall'] = overall_cells[0]\n",
      "        else:\n",
      "            print charity['name'] + \": No Overall cells.\"\n",
      "            charity['cn_overall'] = 0\n",
      "        # include financial rating\n",
      "        if len(fin_cells) == 1:\n",
      "            charity['cn_financial'] = fin_cells[0]\n",
      "        elif len(fin_cells) > 1:\n",
      "            print char['name'] + \": Too many candidate cells for Financial.\"\n",
      "            charity['cn_financial'] = fin_cells[0]\n",
      "        else:\n",
      "            #print \"No Financial cells\"\n",
      "            charity['cn_financial'] = 0\n",
      "        # include accountability rating\n",
      "        if len(acct_cells) == 1:\n",
      "            charity['cn_acct_transp'] = acct_cells[0]\n",
      "        elif len(acct_cells) > 1:\n",
      "            print charity['name'] + \": Too many candidate cells for Accountability & Transparency.\"\n",
      "            charity['cn_acct_transp'] = acct_cells[0]\n",
      "        else:\n",
      "            #print \"No Accountability & Transparency cells\"\n",
      "            charity['cn_acct_transp'] = 0\n",
      "    return dictlist\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "char_with_nav_link = get_char_nav_info(char_with_twitter)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "panda_char = pd.DataFrame(char_with_nav_link)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# make sure to do sudo mysqld_safe from command line first\n",
      "def convert_to_sql(pandadf):\n",
      "    mysqlauth = pd.DataFrame.from_csv('/home/kristy/Documents/auth_codes/mysql_user.csv')\n",
      "    user = mysqlauth.username[0]\n",
      "    password = mysqlauth.password[0]\n",
      "\n",
      "    con = mdb.connect('localhost', user, password, 'charity_data')\n",
      "    with con:\n",
      "        cur = con.cursor()\n",
      "        \n",
      "        # set up table\n",
      "        cur.execute(\"DROP TABLE IF EXISTS charities_sql\")\n",
      "        cur.execute(\"\\\n",
      "            CREATE TABLE charities_sql(\\\n",
      "                id INT PRIMARY KEY AUTO_INCREMENT,\\\n",
      "                disease VARCHAR(255),\\\n",
      "                name VARCHAR(255),\\\n",
      "                address VARCHAR(255),\\\n",
      "                city VARCHAR(255),\\\n",
      "                link VARCHAR(255),\\\n",
      "                bbb_link VARCHAR(255),\\\n",
      "                facebook_link VARCHAR(255),\\\n",
      "                twitter_link VARCHAR(255),\\\n",
      "                cn_link VARCHAR(255),\\\n",
      "                bbb_accred VARCHAR(255),\\\n",
      "                cn_rated VARCHAR(255),\\\n",
      "                cn_overall FLOAT(8,4),\\\n",
      "                cn_financial FLOAT(8,4),\\\n",
      "                cn_acct_transp FLOAT(8,4),\\\n",
      "                facebook_likes INT,\\\n",
      "                twitter_followers INT\\\n",
      "           )\"\\\n",
      "        ) \n",
      "        \n",
      "        char_nav_score_placeholder = 0 # not scraped yet\n",
      "        facebook_likes_placeholder = 0 # not scaped yet\n",
      "        \n",
      "        for idx in range(len(pandadf)):\n",
      "            value_str = (\"\\\"\" + str(pandadf.name[idx]) + \"\\\",\\\"\" + str(pandadf.disease[idx]) + \"\\\",\\\"\" + str(pandadf.address[idx]) + \"\\\",\\\"\" \n",
      "                         + str(pandadf.city[idx]) + \"\\\",\\\"\" + str(pandadf.link[idx]) + \"\\\",\\\"\" + str(pandadf.bbb_link[idx])\n",
      "                         + \"\\\",\\\"\" + str(pandadf.facebook_link[idx]) + \"\\\",\\\"\" + str(pandadf.cn_link[idx]) + \"\\\",\\\"\" \n",
      "                         + str(pandadf.twitter_link[idx]) + \"\\\",\\\"\" + str(pandadf.bbb_accred[idx]) + \"\\\",\\\"\" + str(pandadf.cn_rated[idx])\n",
      "                         + \"\\\",\\\"\" + str(pandadf.cn_overall[idx]) + \"\\\",\\\"\" + str(pandadf.cn_financial[idx]) + \"\\\",\\\"\" + str(pandadf.cn_acct_transp[idx]) \n",
      "                         + \"\\\",\\\"\" + str(facebook_likes_placeholder) + \"\\\",\\\"\" + str(pandadf.twitter_followers[idx]) + \"\\\"\")\n",
      "            cur.execute(\"INSERT INTO charities_sql(name, disease, address, city, link, bbb_link, facebook_link, twitter_link, cn_link, bbb_accred, \\\n",
      "                        cn_rated, cn_overall, cn_financial, cn_acct_transp, facebook_likes, twitter_followers) VALUES(\" + value_str + \")\")\n",
      "        \n",
      "        cur.execute(\"SELECT * FROM charities_sql\")\n",
      "        rows = cur.fetchall()\n",
      "        #for row in rows:\n",
      "            #print row\n",
      "\n",
      "    return rows\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sqlrows = convert_to_sql(panda_char)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/usr/local/lib/python2.7/dist-packages/pymysql/cursors.py:134: Warning: Data truncated for column 'cn_overall' at row 1\n",
        "  result = self._query(query)\n",
        "/usr/local/lib/python2.7/dist-packages/pymysql/cursors.py:134: Warning: Data truncated for column 'cn_financial' at row 1\n",
        "  result = self._query(query)\n",
        "/usr/local/lib/python2.7/dist-packages/pymysql/cursors.py:134: Warning: Data truncated for column 'cn_acct_transp' at row 1\n",
        "  result = self._query(query)\n"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(sqlrows)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 40,
       "text": [
        "3"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sqlrows"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 41,
       "text": [
        "((1,\n",
        "  'colorectal cancer',\n",
        "  'Colon Cancer Alliance',\n",
        "  '1025 Vermont Avenue, Suite 1066',\n",
        "  'Washington, DC 20005',\n",
        "  'http://www.ccalliance.org',\n",
        "  'http://www.give.org/charity-reviews/national/cancer/colon-cancer-alliance-in-washington-dc-19043',\n",
        "  'http://www.facebook.com/pages/Colon-Cancer-Alliance/100438444321?ref=ts',\n",
        "  'http://www.charitynavigator.org/index.cfm?bay=search.summary&orgid=12922',\n",
        "  'http://twitter.com/CCAlliance',\n",
        "  'Accredited Charity',\n",
        "  'Rated',\n",
        "  92.65,\n",
        "  0.0,\n",
        "  0.0,\n",
        "  0,\n",
        "  0),\n",
        " (2,\n",
        "  'colorectal cancer',\n",
        "  'Great Plains Colon Cancer Task Force',\n",
        "  'PO Box 3434',\n",
        "  'Omaha, NE 681030434',\n",
        "  '',\n",
        "  'http://www.bbb.org/nebraska/business-reviews/charity-cancer/great-plains-colon-cancer-task-force-in-omaha-ne-300096192',\n",
        "  '',\n",
        "  'http://www.charitynavigator.org/index.cfm?bay=search.profile&ein=272123378',\n",
        "  '',\n",
        "  '\\nAccredited: No\\n',\n",
        "  'Unrated',\n",
        "  0.0,\n",
        "  0.0,\n",
        "  0.0,\n",
        "  0,\n",
        "  0),\n",
        " (3,\n",
        "  'colorectal cancer',\n",
        "  'Colorectal Cancer Coalition',\n",
        "  '1414 Prince Street, Suite 204',\n",
        "  'Alexandria, VA 22314',\n",
        "  'http://www.FightColorectalCancer.org',\n",
        "  'http://www.give.org/charity-reviews/national/cancer/colorectal-cancer-coalition-in-alexandria-va-5314',\n",
        "  'https://www.facebook.com/FightCRC',\n",
        "  'http://www.charitynavigator.org/index.cfm?bay=search.profile&ein=953102332',\n",
        "  'https://twitter.com/fightcrc',\n",
        "  '\\nAccredited: Yes\\n',\n",
        "  'Unrated',\n",
        "  0.0,\n",
        "  0.0,\n",
        "  0.0,\n",
        "  0,\n",
        "  5433))"
       ]
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(sqlrows)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 42,
       "text": [
        "3"
       ]
      }
     ],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print char_with_nav_link[2]['name']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Colorectal Cancer Coalition\n"
       ]
      }
     ],
     "prompt_number": 49
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}