{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import bs4\n",
      "import re\n",
      "import matplotlib.pyplot as plt\n",
      "import requests, urllib2\n",
      "import tweepy\n",
      "import pymysql as mdb\n",
      "import sqlalchemy\n",
      "\n",
      "%matplotlib inline\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def scrape_bbb_urls():\n",
      "    # import URL into beautiful soup object\n",
      "    give_org_urls = [\"http://give.org/search/?term=multiple+sclerosis&location=&FilterAccredited=false&tobid=\", \n",
      "                     \"http://give.org/search/?page=2&term=multiple+sclerosis&location=&FilterAccredited=false&tobid=\", \n",
      "                     \"http://give.org/search/?page=3&term=multiple+sclerosis&location=&FilterAccredited=false&tobid=\"]\n",
      "    bbb_soups = []\n",
      "    for give_org_url in give_org_urls:\n",
      "        response = requests.get(give_org_url)\n",
      "        bbb_soups.append(bs4.BeautifulSoup(response.text))\n",
      "        \n",
      "    return bbb_soups\n",
      "\n",
      "def clean_bbb_info(bbb_soups):\n",
      "    # pull out charity links and info\n",
      "    # NOTE: May need permission to include links to BBB. I didn't find any info on restrictions in pulling information.\n",
      "    charities = []\n",
      "    for soup in bbb_soups:\n",
      "        bbb_table = soup.select('div.search table')[0].tbody \n",
      "        for row in bbb_table.findAll('tr'):\n",
      "            charity_data = {}\n",
      "            charity_data['name'] = row.select('a.charity-link')[0].get_text()\n",
      "            charity_data['bbb_link'] = row.select('a.charity-link')[0].attrs.get('href')\n",
      "            accred_seal = row.select('img.charity-search-seal')\n",
      "            if accred_seal:\n",
      "                charity_data['bbb_accred'] = accred_seal[0].attrs.get('alt')\n",
      "            else:\n",
      "                charity_data['bbb_accred'] = row.select('div.accreditation-mobile')[0].get_text()\n",
      "            charity_data['address'] = row.select('div.accreditation-mobile')[0].next_sibling.strip()\n",
      "            charity_data['city'] = row.select('div.accreditation-mobile')[0].next_sibling.next_sibling.next_sibling.strip()\n",
      "            \n",
      "            charities.append(charity_data)\n",
      "            \n",
      "    return charities\n",
      "\n",
      "def get_charity_links(charities):\n",
      "    # Use BBB pages to get links to each charity's website\n",
      "    \n",
      "    for charity in charities:\n",
      "        soup = bs4.BeautifulSoup(requests.get(charity['bbb_link']).text)\n",
      "        if len(soup.select('div.charity-contact')) > 0:\n",
      "            charity['link'] = soup.select('div.charity-contact')[0].a.attrs.get('href')\n",
      "        elif len(soup.select('div.charity-detail-text')) > 0:\n",
      "            charity['link'] = soup.select('div.charity-detail-text')[0].a.attrs.get('href')\n",
      "        elif len(soup.select('a#ctl00_ContentPlaceHolder1_aWebUrl')) > 0:\n",
      "            charity['link'] = soup.select('a#ctl00_ContentPlaceHolder1_aWebUrl')[0].attrs.get('href')\n",
      "        else:\n",
      "            charity['link'] = ''\n",
      "    # TO DO: should add text search for websites printed as text rather than link\n",
      "    \n",
      "    return charities\n",
      "\n",
      "def scrape_social_media(charities):\n",
      "    # Find Facebook likes and Twitter followers for each charity\n",
      "    # TO DO: Find the right FB/Twitter account, e.g. for National MS Society chapters.\n",
      "    for charity in charities:\n",
      "        if charity['link'] == '':\n",
      "            charity['facebook_link'] = ''\n",
      "            charity['twitter_link'] = ''\n",
      "        else: \n",
      "            try:\n",
      "                page = urllib2.urlopen(urllib2.Request(charity['link'], headers={'User-Agent' : 'Mozilla/5.0 (Windows; U; Windows NT 5.1; it; rv:1.8.1.11) Gecko/20071127 Firefox/2.0.0.11'}))\n",
      "                target_soup = bs4.BeautifulSoup(page.read(), 'html.parser')\n",
      "            except:\n",
      "                print charity['name']\n",
      "                continue\n",
      "    \n",
      "            try:\n",
      "                fblink = target_soup.select('a[href*=facebook.com]')[0].attrs.get('href')\n",
      "            except IndexError:\n",
      "                fblink = ''\n",
      "            charity['facebook_link'] = fblink \n",
      "            \n",
      "            try:\n",
      "                twlink = target_soup.select('a[href*=twitter.com]')[0].attrs.get('href')\n",
      "            except IndexError:\n",
      "                twlink = ''\n",
      "            charity['twitter_link'] = twlink\n",
      "            \n",
      "    return charities\n",
      "\n",
      "def get_twitter_followers(charities):\n",
      "    # import authorization info \n",
      "    twauth = pd.DataFrame.from_csv('twitter_access.csv')\n",
      "    auth = tweepy.OAuthHandler(twauth.consumer_key['twitter'], twauth.consumer_secret['twitter'])\n",
      "    auth.set_access_token(twauth.access_token['twitter'], twauth.access_secret['twitter'])\n",
      "    \n",
      "    api = tweepy.API(auth)\n",
      "    \n",
      "    for charity in charities:\n",
      "        if charity['twitter_link'] == '':\n",
      "            charity['twitter_followers'] = 0\n",
      "        else:\n",
      "            try:\n",
      "                twitterid = re.findall('https://twitter.com/(.*)', charity['twitter_link'])[0]\n",
      "                user = api.get_user(twitterid)\n",
      "                charity['twitter_followers'] = user.followers_count\n",
      "            except:\n",
      "#                print charity['name']\n",
      "                charity['twitter_followers'] = 0\n",
      "                continue\n",
      "    return charities\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bbb_soups = scrape_bbb_urls()\n",
      "bbb_charities = clean_bbb_info(bbb_soups)\n",
      "charities_with_links = get_charity_links(bbb_charities)\n",
      "charities_with_social = scrape_social_media(charities_with_links)\n",
      "char_with_twitter = get_twitter_followers(charities_with_social)\n",
      "panda_char = pd.DataFrame(char_with_twitter)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mysqlauth = pd.DataFrame.from_csv('mysql_user.csv')\n",
      "user = mysqlauth.username[0]\n",
      "password = mysqlauth.password[0]\n",
      "\n",
      "con = mdb.connect('localhost', user, password, 'charity_data')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with con:\n",
      "    cur = con.cursor()\n",
      "    \n",
      "    # set up table\n",
      "    cur.execute(\"DROP TABLE IF EXISTS charities_sql\")\n",
      "    cur.execute(\"\\\n",
      "        CREATE TABLE charities_sql(\\\n",
      "            id INT PRIMARY KEY AUTO_INCREMENT,\\\n",
      "            name VARCHAR(255),\\\n",
      "            address VARCHAR(255),\\\n",
      "            city VARCHAR(255),\\\n",
      "            link VARCHAR(255),\\\n",
      "            bbb_link VARCHAR(255),\\\n",
      "            facebook_link VARCHAR(255),\\\n",
      "            twitter_link VARCHAR(255),\\\n",
      "            bbb_accred VARCHAR(255),\\\n",
      "            char_nav_score FLOAT(8,4),\\\n",
      "            facebook_likes INT,\\\n",
      "            twitter_followers INT\\\n",
      "       )\"\\\n",
      "    ) \n",
      "    \n",
      "    char_nav_score_placeholder = 0\n",
      "    facebook_likes_placeholder = 0\n",
      "    for idx in range(len(pandadf)):\n",
      "        value_str = (\"\\\"\" + str(pandadf.name[idx]) + \"\\\",\\\"\" + str(pandadf.address[idx]) + \"\\\",\\\"\" \n",
      "                     + str(pandadf.city[idx]) + \"\\\",\\\"\" + str(pandadf.link[idx]) + \"\\\",\\\"\" + str(pandadf.bbb_link[idx])\n",
      "                     + \"\\\",\\\"\" + str(pandadf.facebook_link[idx]) + \"\\\",\\\"\" + str(pandadf.twitter_link[idx]) + \"\\\",\\\"\"\n",
      "                     + str(pandadf.bbb_accred[idx])+ \"\\\",\\\"\" + str(char_nav_score_placeholder) + \"\\\",\\\"\" + str(facebook_likes_placeholder) \n",
      "                     + \"\\\",\\\"\" + str(pandadf.twitter_followers[idx]) + \"\\\"\")\n",
      "        cur.execute(\"INSERT INTO charities_sql(name, address, city, link, bbb_link, facebook_link, twitter_link, bbb_accred, \\\n",
      "                    char_nav_score, facebook_likes, twitter_followers) VALUES(\" + value_str + \")\")\n",
      "    \n",
      "    cur.execute(\"SELECT * FROM charities_sql\")\n",
      "    rows = cur.fetchall()\n",
      "#    for row in rows:\n",
      "#        print row\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 58
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}