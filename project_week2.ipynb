{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import bs4\n",
      "import re, numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import requests, urllib2\n",
      "\n",
      "%matplotlib inline\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def scrape_bbb_urls():\n",
      "    # import URL into beautiful soup object\n",
      "    give_org_urls = [\"http://give.org/search/?term=multiple+sclerosis&location=&FilterAccredited=false&tobid=\", \n",
      "                     \"http://give.org/search/?page=2&term=multiple+sclerosis&location=&FilterAccredited=false&tobid=\", \n",
      "                     \"http://give.org/search/?page=3&term=multiple+sclerosis&location=&FilterAccredited=false&tobid=\"]\n",
      "    bbb_soups = []\n",
      "    for give_org_url in give_org_urls:\n",
      "        response = requests.get(give_org_url)\n",
      "        bbb_soups.append(bs4.BeautifulSoup(response.text))\n",
      "        \n",
      "    return bbb_soups\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def clean_bbb_info(bbb_soups):\n",
      "    # pull out charity links and info\n",
      "    # NOTE: May need permission to include links to BBB. I didn't find any info on restrictions in pulling information.\n",
      "    charities = []\n",
      "    for soup in bbb_soups:\n",
      "        bbb_table = soup.select('div.search table')[0].tbody \n",
      "        for row in bbb_table.findAll('tr'):\n",
      "            charity_data = {}\n",
      "            charity_data['name'] = row.select('a.charity-link')[0].get_text()\n",
      "            charity_data['bbb_link'] = row.select('a.charity-link')[0].attrs.get('href')\n",
      "            accred_seal = row.select('img.charity-search-seal')\n",
      "            if accred_seal:\n",
      "                charity_data['bbb_accred'] = accred_seal[0].attrs.get('alt')\n",
      "            else:\n",
      "                charity_data['bbb_accred'] = row.select('div.accreditation-mobile')[0].get_text()\n",
      "            charity_data['address'] = row.select('div.accreditation-mobile')[0].next_sibling.strip()\n",
      "            charity_data['city'] = row.select('div.accreditation-mobile')[0].next_sibling.next_sibling.next_sibling.strip()\n",
      "            \n",
      "            charities.append(charity_data)\n",
      "            \n",
      "    return charities\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bbb_soups = scrape_bbb_urls()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bbb_charities = clean_bbb_info(bbb_soups)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_charity_links(charities):\n",
      "    # Use BBB pages to get links to each charity's website\n",
      "    \n",
      "    for charity in charities:\n",
      "        soup = bs4.BeautifulSoup(requests.get(charity['bbb_link']).text)\n",
      "        if len(soup.select('div.charity-contact')) > 0:\n",
      "            charity['link'] = soup.select('div.charity-contact')[0].a.attrs.get('href')\n",
      "        elif len(soup.select('div.charity-detail-text')) > 0:\n",
      "            charity['link'] = soup.select('div.charity-detail-text')[0].a.attrs.get('href')\n",
      "        elif len(soup.select('a#ctl00_ContentPlaceHolder1_aWebUrl')) > 0:\n",
      "            charity['link'] = soup.select('a#ctl00_ContentPlaceHolder1_aWebUrl')[0].attrs.get('href')\n",
      "        else:\n",
      "            charity['link'] = ''\n",
      "    # TO DO: should add text search for websites printed as text rather than link\n",
      "    \n",
      "    return charities"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "charities_with_links = get_charity_links(bbb_charities)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def scrape_social_media(charities):\n",
      "    # Find Facebook likes and Twitter followers for each charity\n",
      "    \n",
      "    for charity in charities:\n",
      "        if charity['link'] == '':\n",
      "            charity['facebook_link'] = ''\n",
      "            charity['twitter_link'] = ''\n",
      "        else: \n",
      "            try:\n",
      "                page = urllib2.urlopen(urllib2.Request(charity['link'], headers={'User-Agent' : 'Mozilla/5.0 (Windows; U; Windows NT 5.1; it; rv:1.8.1.11) Gecko/20071127 Firefox/2.0.0.11'}))\n",
      "                target_soup = bs4.BeautifulSoup(page.read(), 'html.parser')\n",
      "            except:\n",
      "                print charity['name']\n",
      "                continue\n",
      "    \n",
      "            try:\n",
      "                fblink = target_soup.select('a[href*=facebook.com]')[0].attrs.get('href')\n",
      "            except IndexError:\n",
      "                fblink = ''\n",
      "            charity['facebook_link'] = fblink \n",
      "            \n",
      "            try:\n",
      "                twlink = target_soup.select('a[href*=twitter.com]')[0].attrs.get('href')\n",
      "            except IndexError:\n",
      "                twlink = ''\n",
      "            charity['twitter_link'] = twlink\n",
      "            \n",
      "    return charities\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 48
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "charities_with_social = scrape_social_media(charities_with_links)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 51
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}