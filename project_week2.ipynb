{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import bs4\n",
      "import re, numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import requests, urllib2\n",
      "import tweepy\n",
      "import pymysql as mdb\n",
      "\n",
      "%matplotlib inline\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 119
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "chosen_disease = \"COLON CANCER\"\n",
      "clean_disease_name = ' '.join(chosen_disease.lower().split())\n",
      "# TO DO: allow synonymous names (e.g. colon vs colorectal cancer)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 120
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def scrape_bbb_urls(disease):\n",
      "    # import URL into beautiful soup object\n",
      "    # TO DO: go through multiple pages of search results\n",
      "    dis_as_str = '+'.join(disease.split())\n",
      "    give_org_urls = [\"http://give.org/search/?term=\" + dis_as_str + \"&location=&FilterAccredited=false&tobid=\"]\n",
      "\n",
      "#    give_org_urls = [\"http://give.org/search/?term=multiple+sclerosis&location=&FilterAccredited=false&tobid=\", \n",
      "#                     \"http://give.org/search/?page=2&term=multiple+sclerosis&location=&FilterAccredited=false&tobid=\", \n",
      "#                     \"http://give.org/search/?page=3&term=multiple+sclerosis&location=&FilterAccredited=false&tobid=\"]\n",
      "    bbb_soups = []\n",
      "    for give_org_url in give_org_urls:\n",
      "        response = requests.get(give_org_url)\n",
      "        bbb_soups.append(bs4.BeautifulSoup(response.text))\n",
      "        \n",
      "    return bbb_soups\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 121
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def clean_bbb_info(bbb_soups,disease):\n",
      "    # pull out charity links and info\n",
      "    # NOTE: May need permission to include links to BBB. I didn't find any info on restrictions in pulling information.\n",
      "    charities = []\n",
      "    for soup in bbb_soups:\n",
      "        bbb_table = soup.select('div.search table')[0].tbody \n",
      "        for row in bbb_table.findAll('tr'):\n",
      "            charity_data = {}\n",
      "            charity_data['disease'] = disease\n",
      "            charity_data['name'] = row.select('a.charity-link')[0].get_text()\n",
      "            charity_data['bbb_link'] = row.select('a.charity-link')[0].attrs.get('href')\n",
      "            accred_seal = row.select('img.charity-search-seal')\n",
      "            if accred_seal:\n",
      "                charity_data['bbb_accred'] = accred_seal[0].attrs.get('alt')\n",
      "            else:\n",
      "                charity_data['bbb_accred'] = row.select('div.accreditation-mobile')[0].get_text()\n",
      "            charity_data['address'] = row.select('div.accreditation-mobile')[0].next_sibling.strip()\n",
      "            charity_data['city'] = row.select('div.accreditation-mobile')[0].next_sibling.next_sibling.next_sibling.strip()\n",
      "            \n",
      "            charities.append(charity_data)\n",
      "            \n",
      "    return charities\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 122
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bbb_soups = scrape_bbb_urls(clean_disease_name)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 123
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(bbb_soups)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 124,
       "text": [
        "1"
       ]
      }
     ],
     "prompt_number": 124
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bbb_charities = clean_bbb_info(bbb_soups,clean_disease_name)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 125
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(bbb_charities)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 126,
       "text": [
        "2"
       ]
      }
     ],
     "prompt_number": 126
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_charity_links(charities):\n",
      "    # Use BBB pages to get links to each charity's website\n",
      "    \n",
      "    for charity in charities:\n",
      "        soup = bs4.BeautifulSoup(requests.get(charity['bbb_link']).text)\n",
      "        if len(soup.select('div.charity-contact')) > 0:\n",
      "            charity['link'] = soup.select('div.charity-contact')[0].a.attrs.get('href')\n",
      "        elif len(soup.select('div.charity-detail-text')) > 0:\n",
      "            charity['link'] = soup.select('div.charity-detail-text')[0].a.attrs.get('href')\n",
      "        elif len(soup.select('a#ctl00_ContentPlaceHolder1_aWebUrl')) > 0:\n",
      "            charity['link'] = soup.select('a#ctl00_ContentPlaceHolder1_aWebUrl')[0].attrs.get('href')\n",
      "        else:\n",
      "            charity['link'] = ''\n",
      "    # TO DO: should add text search for websites printed as text rather than link\n",
      "    \n",
      "    return charities"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 127
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "charities_with_links = get_charity_links(bbb_charities)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 128
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(charities_with_links)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 129,
       "text": [
        "2"
       ]
      }
     ],
     "prompt_number": 129
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def scrape_social_media(charities):\n",
      "    # Find Facebook likes and Twitter followers for each charity\n",
      "    # TO DO: Find the right FB/Twitter account, e.g. for National MS Society chapters.\n",
      "    for charity in charities:\n",
      "        if charity['link'] == '':\n",
      "            charity['facebook_link'] = ''\n",
      "            charity['twitter_link'] = ''\n",
      "        else: \n",
      "            try:\n",
      "                page = urllib2.urlopen(urllib2.Request(charity['link'], headers={'User-Agent' : 'Mozilla/5.0 (Windows; U; Windows NT 5.1; it; rv:1.8.1.11) Gecko/20071127 Firefox/2.0.0.11'}))\n",
      "                target_soup = bs4.BeautifulSoup(page.read(), 'html.parser')\n",
      "            except:\n",
      "                print charity['name']\n",
      "                continue\n",
      "    \n",
      "            try:\n",
      "                fblink = target_soup.select('a[href*=facebook.com]')[0].attrs.get('href')\n",
      "            except IndexError:\n",
      "                fblink = ''\n",
      "            charity['facebook_link'] = fblink \n",
      "            \n",
      "            try:\n",
      "                twlink = target_soup.select('a[href*=twitter.com]')[0].attrs.get('href')\n",
      "            except IndexError:\n",
      "                twlink = ''\n",
      "            charity['twitter_link'] = twlink\n",
      "            \n",
      "    return charities\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 130
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "charities_with_social = scrape_social_media(charities_with_links)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 131
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(charities_with_social)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 132,
       "text": [
        "2"
       ]
      }
     ],
     "prompt_number": 132
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_twitter_followers(charities):\n",
      "    # import authorization info \n",
      "    twauth = pd.DataFrame.from_csv('/home/kristy/Documents/auth_codes/twitter_access.csv')\n",
      "    auth = tweepy.OAuthHandler(twauth.consumer_key['twitter'], twauth.consumer_secret['twitter'])\n",
      "    auth.set_access_token(twauth.access_token['twitter'], twauth.access_secret['twitter'])\n",
      "    \n",
      "    api = tweepy.API(auth)\n",
      "    \n",
      "    for charity in charities:\n",
      "        if charity['twitter_link'] == '':\n",
      "            charity['twitter_followers'] = 0\n",
      "        else:\n",
      "            try:\n",
      "                twitterid = re.findall('https://twitter.com/(.*)', charity['twitter_link'])[0]\n",
      "                user = api.get_user(twitterid)\n",
      "                charity['twitter_followers'] = user.followers_count\n",
      "            except:\n",
      "                charity['twitter_followers'] = 0\n",
      "                continue\n",
      "                \n",
      "    return charities\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 133
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "char_with_twitter = get_twitter_followers(charities_with_social)\n",
      "len(char_with_twitter)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 134,
       "text": [
        "2"
       ]
      }
     ],
     "prompt_number": 134
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_char_nav_info(dictlist): \n",
      "    for charity in dictlist:\n",
      "        # get link to Charity Navigator's website on this charity\n",
      "        words = '+'.join(charity['name'].split())\n",
      "        url = ('https://www.charitynavigator.org/index.cfm?keyword_list=' + words \n",
      "               + '&nameonly=1&Submit2=Search&bay=search.results')\n",
      "        response = requests.get(url)\n",
      "        soup = bs4.BeautifulSoup(response.text)\n",
      "        if len(soup.select('p.rating')) > 0:\n",
      "            charity['cn_link'] = soup.select('p.rating')[0].a.attrs.get('href')\n",
      "            charity['cn_rated'] = 'Rated'\n",
      "        else:\n",
      "            url = ('https://www.charitynavigator.org/index.cfm?keyword_list=' + words \n",
      "                   + '&nameonly=1&Submit2=Search&bay=search.results2')\n",
      "            response = requests.get(url)\n",
      "            soup = bs4.BeautifulSoup(response.text)\n",
      "            if len(soup.select('p.orgname')) > 0:\n",
      "                charity['cn_link'] = soup.select('p.orgname')[0].a.attrs.get('href')\n",
      "                charity['cn_rated'] = 'Unrated'                \n",
      "            else:\n",
      "                print charity['name'] + \": No Charity Navigator link.\"\n",
      "                charity['cn_link'] = ''\n",
      "                charity['cn_rated'] = ''\n",
      "            charity['cn_overall'] = float('nan')\n",
      "            charity['cn_financial'] = float('nan')\n",
      "            charity['cn_acct_transp'] = float('nan')\n",
      "            continue\n",
      "        \n",
      "        # get Charity Navigator data \n",
      "        charsoup = bs4.BeautifulSoup(requests.get(charity['cn_link']).text)\n",
      "        strongtags = charsoup.select('strong')\n",
      "        overall_cells = []\n",
      "        fin_cells = []\n",
      "        acct_cells = []\n",
      "        for tg in strongtags:\n",
      "            if tg.text == \"Overall\":\n",
      "                overall_cells.append(float(tg.parent.nextSibling.nextSibling.text))\n",
      "            elif tg.text == \"Financial\":\n",
      "                fin_cells.append(float(tg.parent.nextSibling.nextSibling.text))\n",
      "            elif tg.text == \"Accountability & Transparency\":\n",
      "                acct_cells.append(float(tg.parent.nextSibling.nextSibling.text))\n",
      "        # include overall rating\n",
      "        if len(overall_cells) == 1:\n",
      "            charity['cn_overall'] = overall_cells[0]\n",
      "        elif len(overall_cells) > 1:\n",
      "            print charity['name'] + \": Too many candidate cells for Overall.\"\n",
      "            charity['cn_overall'] = overall_cells[0]\n",
      "        else:\n",
      "            print charity['name'] + \": No Overall cells.\"\n",
      "            charity['cn_overall'] = 0\n",
      "        # include financial rating\n",
      "        if len(fin_cells) == 1:\n",
      "            charity['cn_financial'] = fin_cells[0]\n",
      "        elif len(fin_cells) > 1:\n",
      "            print char['name'] + \": Too many candidate cells for Financial.\"\n",
      "            charity['cn_financial'] = fin_cells[0]\n",
      "        else:\n",
      "            #print \"No Financial cells\"\n",
      "            charity['cn_financial'] = 0\n",
      "        # include accountability rating\n",
      "        if len(acct_cells) == 1:\n",
      "            charity['cn_acct_transp'] = acct_cells[0]\n",
      "        elif len(acct_cells) > 1:\n",
      "            print charity['name'] + \": Too many candidate cells for Accountability & Transparency.\"\n",
      "            charity['cn_acct_transp'] = acct_cells[0]\n",
      "        else:\n",
      "            #print \"No Accountability & Transparency cells\"\n",
      "            charity['cn_acct_transp'] = 0\n",
      "    return dictlist\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 135
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "char_with_nav_link = get_char_nav_info(char_with_twitter)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 136
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "panda_char = pd.DataFrame(char_with_nav_link)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 137
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# make sure to do sudo mysqld_safe from command line first\n",
      "def convert_to_sql(pandadf,disease):\n",
      "    mysqlauth = pd.DataFrame.from_csv('/home/kristy/Documents/auth_codes/mysql_user.csv')\n",
      "    user = mysqlauth.username[0]\n",
      "    password = mysqlauth.password[0]\n",
      "\n",
      "    con = mdb.connect('localhost', user, password, 'charity_data')\n",
      "    \n",
      "    table_name = '_'.join(disease.split())\n",
      "    print table_name\n",
      "    with con:\n",
      "        cur = con.cursor()\n",
      "        \n",
      "        # set up table\n",
      "#        cur.execute(\"DROP TABLE IF EXISTS charities_sql\")\n",
      "        cur.execute(\"DROP TABLE IF EXISTS \" + str(table_name))\n",
      "        cur.execute(\"\\\n",
      "            CREATE TABLE \" + str(table_name) + \"(\\\n",
      "                id INT PRIMARY KEY AUTO_INCREMENT,\\\n",
      "                disease VARCHAR(255),\\\n",
      "                name VARCHAR(255),\\\n",
      "                address VARCHAR(255),\\\n",
      "                city VARCHAR(255),\\\n",
      "                link VARCHAR(255),\\\n",
      "                bbb_link VARCHAR(255),\\\n",
      "                facebook_link VARCHAR(255),\\\n",
      "                twitter_link VARCHAR(255),\\\n",
      "                cn_link VARCHAR(255),\\\n",
      "                bbb_accred VARCHAR(255),\\\n",
      "                cn_rated VARCHAR(255),\\\n",
      "                cn_overall FLOAT(8,4),\\\n",
      "                cn_financial FLOAT(8,4),\\\n",
      "                cn_acct_transp FLOAT(8,4),\\\n",
      "                facebook_likes INT,\\\n",
      "                twitter_followers INT\\\n",
      "           )\"\\\n",
      "        ) \n",
      "        \n",
      "        facebook_likes_placeholder = 0 # not scaped yet\n",
      "        \n",
      "        for idx in range(len(pandadf)):\n",
      "            value_str = (\"\\\"\" + str(pandadf.name[idx]) + \"\\\",\\\"\" + str(pandadf.disease[idx]) + \"\\\",\\\"\" + str(pandadf.address[idx]) + \"\\\",\\\"\" \n",
      "                         + str(pandadf.city[idx]) + \"\\\",\\\"\" + str(pandadf.link[idx]) + \"\\\",\\\"\" + str(pandadf.bbb_link[idx])\n",
      "                         + \"\\\",\\\"\" + str(pandadf.facebook_link[idx]) + \"\\\",\\\"\" + str(pandadf.cn_link[idx]) + \"\\\",\\\"\" \n",
      "                         + str(pandadf.twitter_link[idx]) + \"\\\",\\\"\" + str(pandadf.bbb_accred[idx]) + \"\\\",\\\"\" + str(pandadf.cn_rated[idx])\n",
      "                         + \"\\\",\\\"\" + str(pandadf.cn_overall[idx]) + \"\\\",\\\"\" + str(pandadf.cn_financial[idx]) + \"\\\",\\\"\" + str(pandadf.cn_acct_transp[idx]) \n",
      "                         + \"\\\",\\\"\" + str(facebook_likes_placeholder) + \"\\\",\\\"\" + str(pandadf.twitter_followers[idx]) + \"\\\"\")\n",
      "            cur.execute(\"INSERT INTO \" + str(table_name) + \"(name, disease, address, city, link, bbb_link, facebook_link, twitter_link, cn_link, bbb_accred, \\\n",
      "                        cn_rated, cn_overall, cn_financial, cn_acct_transp, facebook_likes, twitter_followers) VALUES(\" + value_str + \")\")\n",
      "        \n",
      "        cur.execute(\"SELECT * FROM \" + str(table_name))\n",
      "        rows = cur.fetchall()\n",
      "        #for row in rows:\n",
      "            #print row\n",
      "\n",
      "    return rows\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 138
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sqlrows = convert_to_sql(panda_char,clean_disease_name)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "colon_cancer\n"
       ]
      }
     ],
     "prompt_number": 139
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(sqlrows)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 140,
       "text": [
        "2"
       ]
      }
     ],
     "prompt_number": 140
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print sqlrows"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "((1, 'colon cancer', 'Colon Cancer Alliance', '1025 Vermont Avenue, Suite 1066', 'Washington, DC 20005', 'http://www.ccalliance.org', 'http://www.give.org/charity-reviews/national/cancer/colon-cancer-alliance-in-washington-dc-19043', 'http://www.facebook.com/pages/Colon-Cancer-Alliance/100438444321?ref=ts', 'http://www.charitynavigator.org/index.cfm?bay=search.summary&orgid=12922', 'http://twitter.com/CCAlliance', 'Accredited Charity', 'Rated', 92.65, 0.0, 0.0, 0, 0), (2, 'colon cancer', 'Great Plains Colon Cancer Task Force', 'PO Box 3434', 'Omaha, NE 681030434', '', 'http://www.bbb.org/nebraska/business-reviews/charity-cancer/great-plains-colon-cancer-task-force-in-omaha-ne-300096192', '', 'http://www.charitynavigator.org/index.cfm?bay=search.profile&ein=272123378', '', '\\nAccredited: No\\n', 'Unrated', 0.0, 0.0, 0.0, 0, 0))\n"
       ]
      }
     ],
     "prompt_number": 141
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print clean_disease_name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "colon cancer\n"
       ]
      }
     ],
     "prompt_number": 142
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 142
    }
   ],
   "metadata": {}
  }
 ]
}